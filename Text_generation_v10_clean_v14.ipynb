{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kThrxK5a_ISs",
        "outputId": "d1c77b3c-7d09-4251-8e83-4f0c687ba71b"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/PrithivirajDamodaran/Parrot.git >> None\n",
        "!pip install nltk >> None\n",
        "!pip install rouge >> None\n",
        "!pip install datasets >> None\n",
        "!pip install pyarrow==6.0.1 >> None\n",
        "!pip install datasets==2.3.2 >> None\n",
        "!pip install sentencepiece >> None\n",
        "!pip install transformers >> None\n",
        "!pip install pytorch-lightning >> None\n",
        "!pip install rouge_score >> None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7oTUcqNEKLFb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     /Users/cotangentofzero/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# os\n",
        "import warnings\n",
        "import time\n",
        "import tqdm\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "import sentencepiece\n",
        "from random import shuffle\n",
        "\n",
        "# модели\n",
        "import torch\n",
        "from parrot import Parrot\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from transformers import AutoModelForSequenceClassification, BertTokenizer\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "# метрики\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "#from datasets import load_metric\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.corpus import wordnet\n",
        "#from nltk.translate.ter_score import ter_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "\n",
        "nltk.download('punkt', )\n",
        "nltk.download('wordnet', )\n",
        "nltk.download('all', )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFyLAi9KHs6N"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mlNLbVKv_OcO"
      },
      "outputs": [],
      "source": [
        "full_text = \"\"\"\n",
        "“Wings of Fire: An Autobiography of Abdul Kalam”  is written by Dr. APJ Abdul Kalam and Arun Tiwari. This book is an autobiography of Dr.\n",
        "APJ Abdul Kalam, former president of India. This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist.\n",
        "It reflects how simple living, dedication, strong will and hard work led to success. It also shows how cultural unity impacts the lives of individuals.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOH5HUA7MGVe"
      },
      "source": [
        "## 2. Перефразирование текста по исходному запросу (T5) (+)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTDHehTvMNBs"
      },
      "outputs": [],
      "source": [
        "def create_paraphrase_T5(full_text, max_length, num_return_sequences, early_stopping, type_model):\n",
        "\n",
        "    model_1 = \"t5-base\"\n",
        "    model_2 = \"t5-large\"\n",
        "\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_2)\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_2)\n",
        "\n",
        "    input_text = full_text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    output_ids = model.generate(input_ids,\n",
        "                                max_length=max_length,\n",
        "                                num_return_sequences=num_return_sequences,\n",
        "                                early_stopping=early_stopping)\n",
        "    t5_output_text = tokenizer.decode(output_ids[0],\n",
        "                                      skip_special_tokens=True)\n",
        "\n",
        "    # Вычисление BLEU\n",
        "    bleu_score = sentence_bleu([full_text.split()], t5_output_text.split())\n",
        "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
        "\n",
        "    return t5_output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cs0aTgBN9JT",
        "outputId": "75cf56a2-8016-405c-f500-c4c949764a3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.06\n",
            "Original text: \n",
            "\n",
            "\n",
            "“Wings of Fire: An Autobiography of Abdul Kalam”  is written by Dr. APJ Abdul Kalam and Arun Tiwari. This book is an autobiography of Dr.\n",
            "APJ Abdul Kalam, former president of India. This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist.\n",
            "It reflects how simple living, dedication, strong will and hard work led to success. It also shows how cultural unity impacts the lives of individuals.\n",
            "\n",
            "\n",
            "T5 paraphrased text: \n",
            "\n",
            ". “Wings of Fire” is written by Arun Tiwari. This book is written by Arun Tiwari. This book is written by Arun Tiwari. This book “Wings of Fire” is written by Arun Tiwari. This book is written by Arun Tiwari. “Wings Fire\n",
            "CPU times: user 23.9 s, sys: 3.86 s, total: 27.8 s\n",
            "Wall time: 55 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# max_length, num_return_sequences, early_stopping - добавляем на них крутилки\n",
        "T5_paraphrased = create_paraphrase_T5(full_text, max_length=80, num_return_sequences=1, early_stopping=True)\n",
        "\n",
        "print(\"Original text: \\n\")\n",
        "print(full_text)\n",
        "print(\"\\nT5 paraphrased text: \\n\")\n",
        "print(T5_paraphrased)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6B-GuYHMW9l"
      },
      "source": [
        "## 2. Перефразмирование текста по исходному запросу (Bart) (+)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYNihQUwMdeF"
      },
      "outputs": [],
      "source": [
        "def create_paraphrase_bart(full_text, max_length, num_return_sequences, early_stopping, type_model=\"facebook/bart-large\"):\n",
        "\n",
        "    model = BartForConditionalGeneration.from_pretrained(type_model)\n",
        "    tokenizer = BartTokenizer.from_pretrained(type_model)\n",
        "\n",
        "    input_text = full_text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    output_ids = model.generate(input_ids,\n",
        "                                max_length=max_length,\n",
        "                                num_return_sequences=num_return_sequences,\n",
        "                                early_stopping=early_stopping)\n",
        "    output_text_bart = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Оценка качества перефразирования\n",
        "    bleu_score = sentence_bleu([full_text.split()], output_text_bart.split())\n",
        "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
        "\n",
        "    print(f\"Перефразированный текст: {output_text_bart}\")\n",
        "\n",
        "    return output_text_bart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDgJqCroM8Dy",
        "outputId": "515f9fd1-5153-4f08-9d41-0ecd99f872de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.67\n",
            "Перефразированный текст: “Wings of Fire: An Autobiography of Abdul Kalam”  is written by Dr. APJ AbdulKalam and Arun Tiwari. This book is an autobiography of Dr. A.K.A.APJ Abdul KalAM, former president of India. This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist.It reflects how simple living, dedication, strong will and hard work led to\n",
            "Original text: \n",
            "\n",
            "\n",
            "“Wings of Fire: An Autobiography of Abdul Kalam”  is written by Dr. APJ Abdul Kalam and Arun Tiwari. This book is an autobiography of Dr.\n",
            "APJ Abdul Kalam, former president of India. This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist.\n",
            "It reflects how simple living, dedication, strong will and hard work led to success. It also shows how cultural unity impacts the lives of individuals.\n",
            "\n",
            "\n",
            "Bart paraphrased text: \n",
            "\n",
            "“Wings of Fire: An Autobiography of Abdul Kalam”  is written by Dr. APJ AbdulKalam and Arun Tiwari. This book is an autobiography of Dr. A.K.A.APJ Abdul KalAM, former president of India. This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist.It reflects how simple living, dedication, strong will and hard work led to\n",
            "CPU times: user 10.2 s, sys: 604 ms, total: 10.8 s\n",
            "Wall time: 14.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "# max_length, num_return_sequences, early_stopping - добавляем на них крутилки\n",
        "bart_paraphrased = create_paraphrase_bart(full_text, max_length=100, num_return_sequences=1, early_stopping=True)\n",
        "\n",
        "print(\"Original text: \\n\")\n",
        "print(full_text)\n",
        "print(\"\\nBart paraphrased text: \\n\")\n",
        "print(bart_paraphrased)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HeFD2gPoI5k"
      },
      "source": [
        "## 2. Перефразирование текста по исходному запросу (T5 + Parrot) (+)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evX95ZfEoDDZ"
      },
      "outputs": [],
      "source": [
        "def create_paraphrase(full_text, use_gpu, diversity_ranker, do_diverse, max_return_phrases, max_length, adequacy_threshold, fluency_threshold):\n",
        "    # Использование модели T5 для перефразирования\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "    input_text = full_text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "    output_ids = model.generate(input_ids, max_length=100, num_return_sequences=1, early_stopping=True)\n",
        "    t5_output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Использование библиотеки Parrot для дополнительного перефразирования\n",
        "    parrot = Parrot(model_tag=\"prithivida/parrot_paraphraser_on_T5\", use_gpu=True)\n",
        "\n",
        "    phrases = [x.strip() for x in full_text.split('.')]\n",
        "    output_phrases = []\n",
        "\n",
        "    for phrase in phrases:\n",
        "        if len(phrase) > 1:\n",
        "            para_phrases = parrot.augment(input_phrase=phrase,\n",
        "                                          use_gpu=False,\n",
        "                                          diversity_ranker=\"levenshtein\",\n",
        "                                          do_diverse=False,\n",
        "                                          max_return_phrases=10,\n",
        "                                          max_length=32,\n",
        "                                          adequacy_threshold=0.99,\n",
        "                                          fluency_threshold=0.90)\n",
        "\n",
        "            try:\n",
        "                for para_phrase in para_phrases:\n",
        "                    (x, y) = para_phrase\n",
        "                    x = x[0].upper() + x[1:] # capitalize\n",
        "                    output_phrases.append(x)\n",
        "                    break # just get the first phrase\n",
        "            except:\n",
        "                print(\"Exception occurred with this one.\")\n",
        "\n",
        "    parrot_output_text = \".\".join(output_phrases)\n",
        "\n",
        "    # Оценка качества перефразирования\n",
        "    bleu_score = sentence_bleu([full_text.split()], parrot_output_text.split())\n",
        "    print(f\"BLEU Score: {bleu_score:.2f}\")\n",
        "\n",
        "    return t5_output_text, parrot_output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll0Ni3ehoDBK",
        "outputId": "bcb199ef-96ae-4cc7-aeec-924cb44d7bb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exception occurred with this one.\n",
            "Exception occurred with this one.\n",
            "Exception occurred with this one.\n",
            "Exception occurred with this one.\n",
            "Exception occurred with this one.\n",
            "BLEU Score: 0.00\n",
            "\n",
            "T5 paraphrased text:\n",
            "“Wings of Fire” is a book by Dr. APJ Abdul Kalam and Arun Tiwari. is a collaboration between Dr. APJ Abdul Kalam and Arun Tiwari. Abdul Kalam..\n",
            "\n",
            "Parrot paraphrased text:\n",
            "APJ Abdul Kalam and Arun Tiwari.APJ Abdul Kalam, former president of India\n",
            "CPU times: user 1min 17s, sys: 3.22 s, total: 1min 20s\n",
            "Wall time: 1min 47s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "t5_paraphrased, parrot_paraphrased = create_paraphrase(full_text,\n",
        "\n",
        "                                                       max_return_phrases=10,\n",
        "                                                       max_length=32,\n",
        "                                                       adequacy_threshold=0.99,\n",
        "                                                       fluency_threshold=0.90)\n",
        "\n",
        "print(\"\\nT5 paraphrased text:\")\n",
        "print(t5_paraphrased)\n",
        "print(\"\\nParrot paraphrased text:\")\n",
        "print(parrot_paraphrased)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEJp24uFlsU4"
      },
      "source": [
        "## 2. Перефразирование текста по исходному запросу (Language) (+)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc_3e-4Rlr_P"
      },
      "outputs": [],
      "source": [
        "# Функция для перефразирования текста\n",
        "def paraphrase(text):\n",
        "\n",
        "    # Загрузка моделей машинного перевода\n",
        "    src_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n",
        "    src_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n",
        "\n",
        "    tgt_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")\n",
        "    tgt_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")\n",
        "\n",
        "    # Перевод текста с английского на русский\n",
        "    input_ids = src_tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    output_ids = src_model.generate(input_ids, max_length=100, num_beams=4, early_stopping=True)\n",
        "    paraphrased_text_ru = src_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Перевод обратно с русского на английский\n",
        "    input_ids = tgt_tokenizer.encode(paraphrased_text_ru, return_tensors=\"pt\")\n",
        "    output_ids = tgt_model.generate(input_ids, max_length=100, num_beams=4, early_stopping=True)\n",
        "    paraphrased_text = tgt_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return paraphrased_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grSJUowXl5no"
      },
      "outputs": [],
      "source": [
        "# Пример использования\n",
        "original_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "paraphrased_text = paraphrase(original_text)\n",
        "print(\"Original text:\", original_text)\n",
        "print(\"Paraphrased text:\", paraphrased_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5JeaFrtn--w"
      },
      "source": [
        "## 2. Перефразирование текста по исходному запросу (Русский язык) (+)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ezRNApwoDqF"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = 'cointegrated/rut5-base-paraphraser'\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def paraphrase(text, beams=5, grams=4, do_sample=False):\n",
        "    x = tokenizer(text, return_tensors='pt', padding=True).to(device)\n",
        "    max_size = int(x.input_ids.shape[1] * 1.5 + 10)\n",
        "    out = model.generate(**x, encoder_no_repeat_ngram_size=grams, num_beams=beams, max_length=max_size, do_sample=do_sample)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(paraphrase('Каждый охотник желает знать, где сидит фазан.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Z_L3vmr-Wf"
      },
      "source": [
        "## 2. Перефразирование текста по исходному запросу (Pegasus) (+)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVM3wT2t2284"
      },
      "outputs": [],
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from itertools import product\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_1 = \"tuner007/pegasus_paraphrase\"\n",
        "model_2 = \"google/pegasus-xsum\"\n",
        "\n",
        "# Загрузка модели и токенайзера\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "# Функция перефразирования текста\n",
        "def rephrase_text(text, max_length, num_beams, early_stopping, num_return_sequences):\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=early_stopping,\n",
        "        num_return_sequences=num_return_sequences\n",
        "    )[0]\n",
        "\n",
        "    return tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "# Определение сетки перебора\n",
        "param_grid = {\n",
        "    'max_length': [50, 100, 150, 200],\n",
        "    'num_beams': [2, 4, 8, 16],\n",
        "    'early_stopping': [True, False],\n",
        "    'num_return_sequences': [1, 3, 5]\n",
        "}\n",
        "\n",
        "# Создание всех комбинаций гиперпараметров\n",
        "all_param_combinations = list(product(*param_grid.values()))\n",
        "\n",
        "# Оригинальный текст для перефразирования\n",
        "original_text = \"Artificial intelligence has made significant advancements in recent years, with applications in various fields like healthcare, finance, and transportation.\"\n",
        "\n",
        "# Оценка и сравнение моделей\n",
        "best_score = 0\n",
        "best_params = None\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
        "\n",
        "for params in tqdm(all_param_combinations, desc=\"Evaluating parameter combinations\"):\n",
        "    max_length, num_beams, early_stopping, num_return_sequences = params\n",
        "\n",
        "    # Пропускаем несовместимые комбинации\n",
        "    if num_return_sequences > num_beams:\n",
        "        continue\n",
        "\n",
        "    rephrased_text = rephrase_text(original_text, max_length, num_beams, early_stopping, num_return_sequences)\n",
        "\n",
        "    # Оценка качества перефразирования\n",
        "    rouge_scores = scorer.score(original_text, rephrased_text)\n",
        "    bleu_score = sentence_bleu([original_text.split()], rephrased_text.split())\n",
        "\n",
        "    # Комбинирование метрик\n",
        "    total_score = 0.5 * (rouge_scores['rouge1'].fmeasure + rouge_scores['rouge2'].fmeasure + rouge_scores['rougeL'].fmeasure) + 0.5 * bleu_score\n",
        "\n",
        "    if total_score > best_score:\n",
        "        best_score = total_score\n",
        "        best_params = {\n",
        "            'max_length': max_length,\n",
        "            'num_beams': num_beams,\n",
        "            'early_stopping': early_stopping,\n",
        "            'num_return_sequences': num_return_sequences\n",
        "        }\n",
        "\n",
        "print(f\"Best parameters: {best_params}\")\n",
        "print(f\"Best score: {best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1OX7ZHOt2nA"
      },
      "source": [
        "## 3. Камуфлирование текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "w0ycmJ9qt1q6"
      },
      "outputs": [],
      "source": [
        "def modify_syntax(text):\n",
        "    modified_text = text\n",
        "\n",
        "    # Генерация случайных порогов вероятности\n",
        "    add_spaces_around_punctuation = random.uniform(0.74, 0.9)\n",
        "    remove_extra_punctuation = random.uniform(0.6, 0.8)\n",
        "    replace_question_exclamation = random.uniform(0.5, 0.74)\n",
        "    normalize_ellipsis = random.uniform(0.7, 0.9)\n",
        "    add_spaces_around_non_alphanumeric = random.uniform(0.75, 1.0)\n",
        "\n",
        "    # Добавление пробелов вокруг знаков препинания\n",
        "    if random.random() < add_spaces_around_punctuation:\n",
        "        modified_text = re.sub(r'([,.\\?!])', r' \\1 ', modified_text)\n",
        "        if random.random() < 0.5:\n",
        "            modified_text = re.sub(r' +', ' ', modified_text)\n",
        "\n",
        "    # Удаление лишних знаков препинания\n",
        "    if random.random() < remove_extra_punctuation:\n",
        "        modified_text = re.sub(r'[,\\.]+(?=[\\)\\]\"])', '', modified_text)\n",
        "        if random.random() < 0.3:\n",
        "            modified_text = re.sub(r'[,\\.]+', random.choice([',', '.', '']), modified_text)\n",
        "\n",
        "    # Замена вопросительного знака с восклицательным\n",
        "    if random.random() < replace_question_exclamation:\n",
        "        modified_text = re.sub(r'\\?!', random.choice(['?', '!']), modified_text)\n",
        "\n",
        "    # Нормализация использования многоточий\n",
        "    if random.random() < normalize_ellipsis:\n",
        "        modified_text = re.sub(r'\\.{2,}', '...', modified_text)\n",
        "        if random.random() < 0.4:\n",
        "            modified_text = re.sub(r'\\.{3}', random.choice(['...', '...']), modified_text)\n",
        "\n",
        "    # Добавление пробелов вокруг непечатных символов\n",
        "    if random.random() < add_spaces_around_non_alphanumeric:\n",
        "        modified_text = re.sub(r'([^a-zA-Z0-9])', r' \\1 ', modified_text)\n",
        "        if random.random() < 0.6:\n",
        "            modified_text = re.sub(r' +', ' ', modified_text)\n",
        "\n",
        "    # Удаление лишних пробелов\n",
        "    modified_text = re.sub(r'\\s+', ' ', modified_text)\n",
        "\n",
        "    return modified_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' “ Wings of Fire : An Autobiography of Abdul Kalam ” is written by Dr . APJ Abdul Kalam and Arun Tiwari . This book is an autobiography of Dr . APJ Abdul Kalam , former president of India . This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist . It reflects how simple living , dedication , strong will and hard work led to success . It also shows how cultural unity impacts the lives of individuals . '"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modify_syntax(full_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68pStRUxt1m8",
        "outputId": "ffa3fdc0-ed8c-440a-d895-f502aff57e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Модифицированный текст:\n",
            "Wings of Fire: An Autobiography of Abdul Kalam is written by Dr. APJ Abdul Kalam and Arun Tiwari This book is an autobiography of Dr. APJ Abdul Kalam, former president of India This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist. It reflects how simple living, dedication, strong will and hard work led to success. It also shows how cultural unity impacts the lives of individuals. .\n"
          ]
        }
      ],
      "source": [
        "# Изменениие длины предложений\n",
        "import re\n",
        "def modify_sentence_length(text):\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "\n",
        "    modified_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if len(sentence.split()) > 20:\n",
        "            short_sentences = re.split(r'[,;]+', sentence)\n",
        "            modified_sentences.extend(short_sentences)\n",
        "        else:\n",
        "            if len(modified_sentences) > 0 and len(modified_sentences[-1].split()) < 10:\n",
        "                modified_sentences[-1] += ' ' + sentence.strip()\n",
        "            else:\n",
        "                modified_sentences.append(sentence.strip())\n",
        "\n",
        "    modified_text = '. '.join(modified_sentences) + '.'\n",
        "\n",
        "    return modified_text\n",
        "\n",
        "original_text = \"Wings of Fire: An Autobiography of Abdul Kalam is written by Dr. APJ Abdul Kalam and Arun Tiwari. This book is an autobiography of Dr. APJ Abdul Kalam, former president of India. This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist. It reflects how simple living, dedication, strong will and hard work led to success. It also shows how cultural unity impacts the lives of individuals.\"\n",
        "\n",
        "modified_text = modify_sentence_length(original_text)\n",
        "\n",
        "# print(\"Оригинальный текст:\")\n",
        "# print(original_text)\n",
        "print(\"\\nМодифицированный текст:\")\n",
        "print(modified_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EANHWNbqt1k5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Модифицированный текст:\n",
            "Wings An by written of Kalam is of Fire: Autobiography Abdul Dr. APJ Abdul Arun Kalam and Tiwari. This an of book is autobiography Dr. APJ president of former Abdul Kalam, India. This a journey boy story from a to the renowned of become young light on Rameswaram sheds scientist. It reflects will and living, work to dedication, strong led simple how hard success. It of cultural unity shows how impacts the lives also individuals. .\n"
          ]
        }
      ],
      "source": [
        "# Перестановка слов\n",
        "def modify_word_order(text):\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "\n",
        "    modified_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "\n",
        "        if len(words) > 2:\n",
        "            middle_words = words[1:-1]\n",
        "            shuffle(middle_words)\n",
        "\n",
        "            modified_sentence = ' '.join([words[0]] + middle_words + [words[-1]])\n",
        "        else:\n",
        "            modified_sentence = ' '.join(words)\n",
        "\n",
        "        modified_sentences.append(modified_sentence)\n",
        "\n",
        "    modified_text = '. '.join(modified_sentences) + '.'\n",
        "\n",
        "    return modified_text\n",
        "\n",
        "original_text = \"Wings of Fire: An Autobiography of Abdul Kalam is written by Dr. APJ Abdul Kalam and Arun Tiwari. This book is an autobiography of Dr. APJ Abdul Kalam, former president of India. This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist. It reflects how simple living, dedication, strong will and hard work led to success. It also shows how cultural unity impacts the lives of individuals.\"\n",
        "\n",
        "modified_text = modify_word_order(original_text)\n",
        "\n",
        "# print(\"Оригинальный текст:\")\n",
        "# print(original_text)\n",
        "print(\"\\nМодифицированный текст:\")\n",
        "print(modified_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xEVUGo2ly9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Предложения в активном залоге, как правило, просто понимаются. Кроме того, добавление в текст слов, передающих эмоции или личное отношение автора, может сделать его более активным и выражительным.\n"
          ]
        }
      ],
      "source": [
        "# Замена пассивных конструкций на активные\n",
        "def activate_passive_sentences(text):\n",
        "    active_text = re.sub(r'(\\w+) was (\\w+)', r'\\2 \\1', text)\n",
        "    return active_text\n",
        "\n",
        "# Добавление эмоциональных/оценочных элементов\n",
        "def add_emotional_elements(text):\n",
        "    emotional_words = [\n",
        "        'весело', 'радостно', 'волнушкой', 'смеяться',\n",
        "        'возбужденно', 'вдохновляюще', 'интригующе', 'взрывно',\n",
        "        'трепетом', 'волнением', 'страстно', 'горячо', 'влюбленно'\n",
        "    ]\n",
        "\n",
        "    for word in emotional_words:\n",
        "        text = re.sub(r'\\bкроме того\\b', f'{word} ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Изменение стиля изложения на более разговорный\n",
        "def change_style_to_conversational(text):\n",
        "    conversational_phrases = {\n",
        "        'формальный': 'разговорный',\n",
        "        'предложения': 'бабочки',\n",
        "        'легче': 'просто',\n",
        "        'воспринимаются': 'понимаются',\n",
        "        'включение': 'добавление',\n",
        "        'живым': 'активным',\n",
        "        'выразительным': 'выражительным',\n",
        "        'кроме того': 'еще',\n",
        "        'также': 'а еще',\n",
        "        'дополнительно': 'и так далее',\n",
        "        'конечно': 'ну конечно',\n",
        "        'действительно': 'правильно',\n",
        "        'по сути': 'в общем',\n",
        "        'в основном': 'в основном'\n",
        "    }\n",
        "\n",
        "    for key, value in conversational_phrases.items():\n",
        "        text = text.replace(key, value)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Пример исходного текста\n",
        "original_text = \"Предложения в активном залоге, как правило, легче воспринимаются. Кроме того, включение в текст слов, передающих эмоции или личное отношение автора, может сделать его более живым и выразительным.\"\n",
        "\n",
        "# Применяем функции к тексту\n",
        "activated_text = activate_passive_sentences(original_text)\n",
        "emotional_text = add_emotional_elements(activated_text)\n",
        "conversational_text = change_style_to_conversational(emotional_text)\n",
        "\n",
        "print(conversational_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VQDfRh8G0gQn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "яПлдежинеро в активном залоге, как правило, легче воспринимаются.\n"
          ]
        }
      ],
      "source": [
        "# Случайные перестановки букв в словах\n",
        "def scramble_words_with_control(text, probability):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    for i, word in enumerate(words):\n",
        "        if random.random() < probability:\n",
        "            scrambled_word = ''.join(random.sample(word, len(word)))\n",
        "            text = text.replace(word, scrambled_word)\n",
        "    return text\n",
        "\n",
        "# Пример использования функции\n",
        "probability=0.1\n",
        "text_with_scramble = \"Предложения в активном залоге, как правило, легче воспринимаются.\"\n",
        "controlled_scrambled_text = scramble_words_with_control(text_with_scramble, probability=probability)\n",
        "\n",
        "print(controlled_scrambled_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "gT_nv_RGJ1Pc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "backstage of Fire: AN autobiography of Abdul Kalam is write by Dr. APJ Abdul Kalam and Arun Tiwari. This book is AN autobiography of doctor APJ Abdul Kalam\n"
          ]
        }
      ],
      "source": [
        "# Замена на синонимы\n",
        "def find_synonyms(word):\n",
        "    synonyms = []\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.append(lemma.name())\n",
        "    return synonyms\n",
        "\n",
        "def replace_synonyms(text):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.lower() in wordnet.words():\n",
        "            synonyms = find_synonyms(word)\n",
        "            if len(synonyms) > 0:\n",
        "                new_word = random.choice(synonyms)\n",
        "                new_words.append(new_word)\n",
        "            else:\n",
        "                new_words.append(word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Пример использования\n",
        "text = \"Wings of Fire: An Autobiography of Abdul Kalam is written by Dr. APJ Abdul Kalam and Arun Tiwari. This book is an autobiography of Dr. APJ Abdul Kalam\"\n",
        "print(replace_synonyms(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4Tukb3L4TFb"
      },
      "source": [
        "Pegasus + Paws + T5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43pzvQl14RqP"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentencepiece >> None\n",
        "!pip install git+https://github.com/PrithivirajDamodaran/Parrot_Paraphraser.git >> None\n",
        "\n",
        "from transformers import *\n",
        "\n",
        "# models we gonna use for this tutorial\n",
        "model_names = [\n",
        "  \"tuner007/pegasus_paraphrase\",\n",
        "  \"Vamsi/T5_Paraphrase_Paws\",\n",
        "  \"prithivida/parrot_paraphraser_on_T5\", # Parrot\n",
        "]\n",
        "\n",
        "model = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
        "tokenizer = PegasusTokenizerFast.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
        "\n",
        "def get_paraphrased_sentences(model, tokenizer, sentence, num_return_sequences=5, num_beams=5):\n",
        "  inputs = tokenizer([sentence], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    num_beams=num_beams,\n",
        "    num_return_sequences=num_return_sequences,\n",
        "  )\n",
        "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "sentence = \"Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences.\"\n",
        "\n",
        "get_paraphrased_sentences(model, tokenizer, sentence, num_beams=10, num_return_sequences=10)\n",
        "\n",
        "get_paraphrased_sentences(model, tokenizer, \"To paraphrase a source, you have to rewrite a passage without changing the meaning of the original text.\", num_beams=10, num_return_sequences=10)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
        "\n",
        "get_paraphrased_sentences(model, tokenizer, \"paraphrase: \" + \"One of the best ways to learn is to teach what you've already learned\")\n",
        "\n",
        "from parrot import Parrot\n",
        "\n",
        "parrot = Parrot()\n",
        "\n",
        "phrases = [\n",
        "  sentence,\n",
        "  \"One of the best ways to learn is to teach what you've already learned\",\n",
        "  \"Paraphrasing is the process of coming up with someone else's ideas in your own words\"\n",
        "]\n",
        "\n",
        "for phrase in phrases:\n",
        "  print(\"-\"*100)\n",
        "  print(\"Input_phrase: \", phrase)\n",
        "  print(\"-\"*100)\n",
        "  paraphrases = parrot.augment(input_phrase=phrase)\n",
        "  if paraphrases:\n",
        "    for paraphrase in paraphrases:\n",
        "      print(paraphrase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SANTrxyOLLV8"
      },
      "source": [
        "## 3. Метрика сравнения сгенеренного текста и исходного"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXv0UlLc3h1T"
      },
      "outputs": [],
      "source": [
        "model_name = 'cointegrated/rubert-base-cased-dp-paraphrase-detection'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Use CUDA if available, otherwise use CPU\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def compare_texts(text1, text2):\n",
        "    batch = tokenizer(text1, text2, return_tensors='pt').to(device)\n",
        "    with torch.inference_mode():\n",
        "        proba = torch.softmax(model(**batch).logits, -1).cpu().numpy()\n",
        "    return proba[0] # p(non-paraphrase), p(paraphrase)\n",
        "\n",
        "print(compare_texts('Сегодня на улице хорошая погода', 'Сегодня на улице отвратительная погода'))\n",
        "print(compare_texts('Сегодня на улице хорошая погода', 'Отличная погодка сегодня выдалась'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "YOH5HUA7MGVe",
        "N6B-GuYHMW9l",
        "7HeFD2gPoI5k",
        "WEJp24uFlsU4",
        "F5JeaFrtn--w",
        "SANTrxyOLLV8"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
