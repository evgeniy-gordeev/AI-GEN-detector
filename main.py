# os
import warnings
import time
import tqdm
import random
import string
import re
import csv
from pathlib import Path
from tqdm import tqdm
import argparse
import glob
import os
import json
import logging
from itertools import chain
from string import punctuation

# –º–æ–¥–µ–ª–∏
import torch
#from parrot import Parrot
from transformers import BartForConditionalGeneration, BartTokenizer
from transformers import T5ForConditionalGeneration, T5Tokenizer
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import MarianMTModel, MarianTokenizer
from concurrent.futures import ThreadPoolExecutor, as_completed

# –º–µ—Ç—Ä–∏–∫–∏
from nltk.translate.bleu_score import sentence_bleu
from datasets import load_metric
import nltk
#from nltk.translate.ter_score import ter_score
import sentencepiece

warnings.filterwarnings("ignore")
nltk.download('punkt')
nltk.download('wordnet')

import streamlit as st
import torch
from transformers import pipeline

def set_seed(seed):
  torch.manual_seed(seed)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

set_seed(42)


def create_paraphrase_T5(full_text, max_length, num_return_sequences, early_stopping, type_model):

    model_1 = "t5-base"
    model_2 = "t5-large"

    model = T5ForConditionalGeneration.from_pretrained(model_2)
    tokenizer = T5Tokenizer.from_pretrained(model_2)

    input_text = full_text
    input_ids = tokenizer.encode(input_text, return_tensors='pt')

    output_ids = model.generate(input_ids,
                                max_length=max_length,
                                num_return_sequences=num_return_sequences,
                                early_stopping=early_stopping)
    t5_output_text = tokenizer.decode(output_ids[0],
                                      skip_special_tokens=True)

    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ BLEU
    bleu_score = sentence_bleu([full_text.split()], t5_output_text.split())
    print(f"BLEU Score: {bleu_score:.2f}")

    return t5_output_text


def is_generated_by_ai(text):
    if text == '':
        return ""
    else:
        text_classifier = pipeline("text-classification", model="Juner/AI-generated-text-detection")
        result = text_classifier(text)[0]
        if result['label'] == "LABEL_1":
            result['label'] = "Human-generated"
            result['score'] = 1 - result['score']
        else:
            result['label'] =  "Human-generated"
            result['score'] = result['score']
        return result

st.set_page_config(
    page_title="AI GEN APP",
    page_icon="üìà",
    menu_items={
        'Get Help': 'https://www.extremelycoolapp.com/help',
        'Report a bug': "https://www.extremelycoolapp.com/bug",
        'About': "# This is a header. This is an *extremely* cool app!"
    }
)

st.markdown("""
# AI GENERERATED TEXT RECOGNITION (AGTR)

AGTR provides one usefull service - Understanding if the text was generated by AI. It utilizes HuggingFace transformer model to solve the task. 

## Enter your text and see the magic!

""")


# max_length, num_return_sequences, early_stopping - –¥–æ–±–∞–≤–ª—è–µ–º –Ω–∞ –Ω–∏—Ö –∫—Ä—É—Ç–∏–ª–∫–∏
full_text = """ 
            ‚ÄúWings of Fire: An Autobiography of Abdul Kalam‚Äù  is written by Dr. APJ Abdul Kalam and Arun Tiwari. This book is an autobiography of Dr.
            APJ Abdul Kalam, former president of India. This story sheds light on the journey of a young boy from Rameswaram to become a renowned scientist.
            It reflects how simple living, dedication, strong will and hard work led to success. It also shows how cultural unity impacts the lives of individuals.
            """
T5_paraphrased = create_paraphrase_T5(full_text, max_length=80, num_return_sequences=1, early_stopping=True)

print("Original text: \n")
print(full_text)
print("\nT5 paraphrased text: \n")
st.write(f'**{T5_paraphrased}**')




user_input = st.text_area('Enter text','', placeholder='–≠—Ç–æ—Ç —Ç–µ–∫—Å—Ç –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏')
output = is_generated_by_ai(user_input)

st.write("Paraphrased Text: ")
st.write(f'**{output}**')